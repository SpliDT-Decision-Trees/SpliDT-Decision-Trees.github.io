{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"benchmarks/","title":"Gigaflow Benchmarking Guide","text":"<p>This guide explains how to evaluate Gigaflow against Megaflow cache using real-world vSwitch pipelines and traffic traces.</p>"},{"location":"benchmarks/#experiment-setup","title":"Experiment Setup","text":"<p>If you haven't completed the last step of installation, then you need to setup the experiment first. Inside the Ansible container, run the following command to install the datasets (pipelines and traffic traces) on the GVS and TGEN machines.</p> Ansible Container<pre><code>make setup-gvs-experiment\n</code></pre> <p>This command will also install gvs and tgen along with all their dependencies on the respective machines.</p>"},{"location":"benchmarks/#option-1-run-all-experiments","title":"Option 1. Run All Experiments","text":"<p>To run all experiments (end-to-end and microbenchmarks) and collect logs, run the following command:</p> Ansible Container<pre><code>make run-gvs-experiment\n</code></pre>"},{"location":"benchmarks/#option-2-end-to-end-evals","title":"Option 2. End-to-End Evals","text":"<p>To setup and run only end-to-end experiments:</p> Ansible Container<pre><code>make run-gvs-ee-experiment\n</code></pre>"},{"location":"benchmarks/#option-3-microbenchmarks","title":"Option 3. Microbenchmarks","text":"<p>To setup and run only microbenchmark experiments:</p> Ansible Container<pre><code>make run-gvs-bm-experiment\n</code></pre>"},{"location":"benchmarks/#option-4-custom-experiment","title":"Option 4. Custom Experiment","text":"<p>To setup and run a specific experiment (with a given locality, pipeline, and Gigaflow tables configuration), modify the following variables in vars/main.yml.</p>"},{"location":"benchmarks/#config-1-locality","title":"Config 1: Locality","text":"<p>The locality (high/low) to generate the correct traffic load. </p> vars/main.yml<pre><code>locality_dynamic:\n  current:\n    locality: \"high-locality\"\n</code></pre> <p>Choose an option from <code>locality_static</code>. The other available options are as following:</p> vars/main.yml<pre><code>locality_static:\n  all:\n    - locality: \"high-locality\"\n    - locality: \"low-locality\"\n</code></pre>"},{"location":"benchmarks/#config-2-vswitch-pipeline","title":"Config 2: vSwitch Pipeline","text":"<p>The pipeline to install in the vSwitch and send traffic for.</p> vars/main.yml<pre><code>pipelines_dynamic: \n  current: \n    name: \"cord-ofdpa\"\n    sub_path: \"cord/ofdpa\"\n</code></pre> <p>Choose an option from <code>pipelines_static</code>. Other available options are as following:</p> vars/main.yml<pre><code>pipelines_static:\n  all:\n    - name: \"antrea-ovs\"\n      sub_path: \"antrea/ovs\"\n    - name: \"ovn-logical-switch\"\n      sub_path: \"ovn/logical-switch\"\n    - name: \"pisces-l2l3-acl\"\n      sub_path: \"pisces/l2l3-acl\"\n    - name: \"cord-ofdpa\"\n      sub_path: \"cord/ofdpa\"\n    - name: \"openflow-ttp-l2l3-acl\"\n      sub_path: \"openflow-ttp/l2l3-acl\"\n</code></pre>"},{"location":"benchmarks/#config-3-gigaflow-tables","title":"Config 3: Gigaflow Tables","text":"<p>The number of Gigaflow tables and entries in each of them.</p> vars/main.yml<pre><code>gigaflow_dynamic:\n  experiment: \"ee\" # this is just the name for the logs directory\n  options:\n      gigaflow_tables_limit: 4\n      gigaflow_max_entries: 8000\n</code></pre> <p>Choose an option from <code>gigaflow_static</code>. Other available options are as following:</p> vars/main.yml<pre><code>gigaflow_static:\n  ee:\n    - gigaflow_tables_limit: 1\n      gigaflow_max_entries: 32000\n    - gigaflow_tables_limit: 4\n      gigaflow_max_entries: 8000\n  bm:\n    - gigaflow_tables_limit: 1\n      gigaflow_max_entries: 100000\n    - gigaflow_tables_limit: 2\n      gigaflow_max_entries: 100000\n    - gigaflow_tables_limit: 3\n      gigaflow_max_entries: 100000\n    - gigaflow_tables_limit: 4\n      gigaflow_max_entries: 100000\n    - gigaflow_tables_limit: 5\n      gigaflow_max_entries: 100000\n</code></pre> <p>Once these variables are setup, run the following sequence of commands. </p> Ansible Container<pre><code>make resetup-tgen-scripts\nmake start-switch-gvs \nmake install-rules\nmake start-tgen\nmake stop-tgen\nmake uninstall-rules \nmake stop-switch-gvs\nmake collect-logs\n</code></pre>"},{"location":"benchmarks/#experiment-teardown","title":"Experiment Teardown","text":"<p>To stop the experiment and remove all installed components, run the following command:</p> Ansible Container<pre><code>make teardown-gvs-experiment\n</code></pre>"},{"location":"benchmarks/#documentation","title":"Documentation","text":"<ul> <li>Getting Started</li> <li>Technical Details</li> <li>Installation Instructions</li> <li>Usage Instructions</li> </ul>"},{"location":"contributing/","title":"Contributing to SpliDT","text":"<p>We welcome contributions to SpliDT! This guide explains how to contribute effectively to the project.</p>"},{"location":"contributing/#contributing-code","title":"Contributing Code","text":""},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Create a feature branch for the specific repo: <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes and commit: <pre><code>git commit -m \"Description of changes\"\n</code></pre></p> </li> <li> <p>Update documentation if needed:</p> <ul> <li>New features</li> <li>Configuration options</li> <li>Performance implications</li> </ul> </li> <li> <p>Submit a pull request</p> </li> </ol>"},{"location":"contributing/#pull-request-checklist","title":"Pull Request Checklist","text":"<p>Before submitting your pull request, please ensure:</p> <ul> <li>Documentation is updated</li> <li>Performance impact has been considered</li> <li>Backwards compatibility is maintained</li> <li>Code follows style guidelines</li> <li>All tests pass</li> </ul>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>When reporting a bug, please include:</p> <ul> <li>SpliDT version (or better yet commit number)</li> <li>System configuration</li> <li>Network environment</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Relevant logs</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>When requesting a feature, please describe:</p> <ul> <li>Use case</li> <li>Expected benefits</li> <li>Performance requirements</li> <li>Resource implications</li> </ul>"},{"location":"contributing/#communication","title":"Communication","text":"<p>We have several channels for communication:</p> <ul> <li>GitHub Issues: Bug reports and feature requests</li> <li>Email: Technical discussions and security reports</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#getting-started","title":"Getting Started","text":"<p>SpliDT is a switch-native compiler framework for deploying real-time ML decision logic into the data plane of programmable switches. It compiles high-performance decision tree models to enable detection and observability of  security-significant flow behaviors across diverse traffic workloads.</p>"},{"location":"getting-started/#why-splidt","title":"Why SpliDT?","text":""},{"location":"getting-started/#scales-model-capacity-flows","title":"\ud83c\udfc6 Scales Model Capacity &amp; Flows","text":"<ul> <li>Up to 5\u00d7 more stateful features than prior in-network DT systems without exceeding switch hardware limits, enabled via partitioned inference and resource reuse.</li> <li>Line-rate inference at scale, supporting millions of concurrent flows with \u2264 0.05% recirculation overhead, preserving data-plane throughput.</li> <li>No loss in detection latency, maintaining the same low time-to-detection (TTD) as prior approaches while achieving significantly higher accuracy across multiple real-world datasets.</li> </ul>"},{"location":"getting-started/#key-features","title":"\ud83d\udca1 Key Features","text":"<ul> <li>Design Search Exploration: Automated design-space exploration using Bayesian optimization</li> <li>Custom Training Framework: Custom hardware-aware training framework for partitioned decision trees</li> <li>Window-based Feature Extraction: Augmented CICFlowMeter with window-based bidirectional feature extraction</li> <li>P4 Code Generation: Automated P4 pipeline generation for partitioned decision-tree inference</li> </ul>"},{"location":"getting-started/#technical-highlights","title":"\ud83d\udd27 Technical Highlights","text":"<ul> <li>Multi-table cache architecture enabling efficient matching across a significantly expanded rule space</li> <li>Effeciently captures pipeline-aware locality by maximizing sub-traversal level disjointedness</li> <li>SmartNIC offload available for Xilinx Alveo U250 Data Center Accelerator (coming soon!)</li> </ul>"},{"location":"getting-started/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"getting-started/#testbed-setup","title":"\ud83c\udfd7\ufe0f Testbed Setup","text":"<p>To set up a testbed for SpliDT and do all necessary installations, follow the steps provided in the Installation Guide. We also provide automatically installed training and testing modules that support SpliDT model development and evaluation.</p>"},{"location":"getting-started/#bootstrap-splidt","title":"\ud83c\udfc1 Bootstrap SpliDT","text":"<p>If you haven't completed the last step of installation, then you need to setup the experiment first. Inside the Ansible container, run the following command to install the datasets (pipelines and traffic traces) on the GVS and TGEN machines.</p> Ansible Container<pre><code>make setup-gvs-experiment\n</code></pre> <p>This command will also install gvs and tgen along with all their dependencies on the respective machines.</p>"},{"location":"getting-started/#run-all-benchmarks","title":"\ud83d\udcc8 Run All Benchmarks","text":"<p>To run all experiments, including end-to-end and microbenchmarks, and collect performance logs, run the following command:</p> <p>Ansible Container<pre><code>make run-gvs-experiment\n</code></pre> See our benchmarking guide for various options to evaluate Gigaflow against Megaflow cache using real-world workloads.</p>"},{"location":"getting-started/#next-steps-with-gigaflow","title":"\ud83e\udde9 Next Steps with Gigaflow","text":""},{"location":"getting-started/#learn-more","title":"\ud83d\udcda Learn More","text":"<ul> <li>Check Usage Guide for detailed configuration options</li> <li>Checkout the Benchmarking Guide for in-depth performance evaluation</li> <li>Read Technical Details to learn about Gigaflow's architecture</li> </ul>"},{"location":"getting-started/#optimizing-performance","title":"\ud83d\udee0\ufe0f Optimizing Performance","text":"<ul> <li>Follow our Benchmarking Guide for in-depth performance evaluation</li> <li>Learn how to emulate high/low locality environments</li> <li>Comprehensively evaluate Gigaflow against traditional Megaflow cache</li> </ul>"},{"location":"getting-started/#getting-help","title":"\ud83d\udedf Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the documentation pages linked above</li> <li>Review existing GitHub issues</li> <li>Open a new issue with a minimal example</li> </ol>"},{"location":"getting-started/#contributing","title":"\ud83e\uddd1\u200d\ud83d\udcbb Contributing","text":"<p>We welcome contributions to SpliDT! Whether it's improving documentation, fixing bugs, optimizing performance, or adding new features, your help is appreciated. Please check our Contributing Guide for guidelines on how to get started.</p>"},{"location":"getting-started/#research","title":"\ud83d\udcc4 Research","text":"<p>Gigaflow was presented at ASPLOS'25. </p> <p>\ud83d\udcc4 Gigaflow: Pipeline-Aware Sub-Traversal Caching for Modern SmartNICs</p> <p> </p> <p>More details on our Publications page.</p>"},{"location":"getting-started/#support","title":"\ud83e\uddd1\u200d\ud83d\udcbc Support","text":"<p>Need help? Here are your options:</p> <ul> <li>Review all the documentation</li> <li>Open an issue</li> <li>Review installation guide for setup help</li> </ul>"},{"location":"home/","title":"SpliDT","text":""},{"location":"installation/","title":"Installation Guide","text":"<p>This guide describes how to install and run SpliDT, a partitioned decision tree training and inference framework designed for scalable, line-rate deployment.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>SpliDT is designed to be lightweight and portable. The only required system dependencies are:</p> <ul> <li>Linux (tested on Ubuntu 20.04 / 22.04)</li> <li>Conda (Miniconda or Anaconda)</li> <li>Git</li> </ul>"},{"location":"installation/#conda-environment-setup","title":"Conda Environment Setup","text":"<p>All software dependencies are specified in <code>environment.yml</code>.</p> <p>Step 1: Create the environment <pre><code>conda env create -f environment.yml\n</code></pre></p> <p>Step 2: Activate the environment <pre><code>conda activate splidt\n</code></pre></p> <p>Verify the environment: <pre><code>python --version\n</code></pre></p>"},{"location":"installation/#datasets","title":"Datasets","text":"<p>To evaluate SpliDT, we release seven real-world datasets that are used for training and evaluating partitioned decision trees under diverse traffic and security conditions. The following are their detailed descriptions (more details in the paper).</p> Dataset Description Classes CIC-IoMT2024 A cybersecurity dataset with Internet of Medical Things (IoMT) traffic for intrusion detection in healthcare. 19 CIC-IoT2023-a A simplified version of the CIC-IoT-2023 dataset, categorized into four primary classes of IoT traffic. 4 ISCX-VPN2016 A dataset containing VPN and non-VPN traffic for evaluating VPN detection and privacy-related analyses. 13 CampusTraffic UCSB campus dataset containing various application types, including web, cloud, social, and streaming traffic. 11 CIC-IoT2023-b A comprehensive IoT dataset containing multi-class network traffic data for evaluating IoT security threats. 32 CIC-IDS2017 A network intrusion detection dataset for various attack scenarios, including DoS, DDoS, and brute force. 10 CIC-IDS2018 An anomaly detection dataset capturing network traffic for diverse attacks and benign activities. 10"},{"location":"installation/#download","title":"Download","text":"<p>The datasets used to train and evaluate SpliDT are publicly available via FigShare. Download and place them on the machine as following:</p> <p>Step 1: create directory shell<pre><code>mkdir ~/splidt\ncd ~/splidt\n</code></pre></p> <p>Step 2: download the traffic traces and pipelines shell<pre><code># download the Datasets for training and testing\ncurl -L -H \"User-Agent: Mozilla/5.0\" -OJ \"https://ndownloader.figshare.com/files/60570641\"\n</code></pre></p> <p>Step 3: unzip the downloaded files shell<pre><code>unzip splidt-dataset\n</code></pre></p> <p>Step 4: Modify the <code>path</code> field in each dataset configuration file under <code>configs/</code> to reference the absolute path of the unzipped dataset directory (e.g., <code>/home/splidt/splidt-dataset</code>). shell<pre><code>path: \"/home/splidt/splidt-dataset\"\n</code></pre></p> <p>Step 5: Clone the HyperMapper repository, then modify the <code>script_path</code> field in each dataset configuration file under <code>configs/</code> to reference the absolute path of the hypermapper.  shell<pre><code>script_path: \"/home/splidt/hypermapper/scripts/hypermapper.py\"\n</code></pre></p> <p>Now, we are ready to train and test our datasets. </p>"},{"location":"installation/#training-setup","title":"Training Setup","text":"<p>You only need to clone the splidt repository that will bringup the testbed, install all dependencies (including <code>gvs</code> and traffic generator), and run the experiments.  The orchestration is enabled via Ansible which itself is provided as a docker container.</p> <p>Note</p> <p>All the steps from this point onwards must be run on your orchestrator machine. For our experiments, we used the <code>gvs</code> machine as our orchestrator but you can choose a different machine too as long as it has <code>docker</code> installed.</p> <p>Clone the gigaflow-orchestrator repository as following:</p> shell<pre><code>git clone https://github.com/gigaflow-vswitch/gigaflow-orchestrator.git\n</code></pre>"},{"location":"installation/#directory-structure","title":"Directory Structure","text":"<pre><code>gigaflow-orchestrator/\n\u251c\u2500\u2500 roles/          # Ansible roles for all components needed to run GvS experiments\n\u2502   \u251c\u2500\u2500 collector/\n\u2502   \u251c\u2500\u2500 dpdk/\n\u2502   \u251c\u2500\u2500 gvs/\n\u2502   \u251c\u2500\u2500 logging/\n\u2502   \u251c\u2500\u2500 retrieve/\n\u2502   \u251c\u2500\u2500 rules/\n\u2502   \u2514\u2500\u2500 tgen/\n\u251c\u2500\u2500 scripts/        \n\u251c\u2500\u2500 vars/           # Experiment variables\n\u251c\u2500\u2500 inventory.ini   # Ansible inventory file\n\u251c\u2500\u2500 ansible.cfg     # Ansible configuration file\n\u251c\u2500\u2500 Makefile        # Makefile for the ansible playbook targets\n\u251c\u2500\u2500 gvs.yml         # top-level gvs ansible playbook\n\u251c\u2500\u2500 tgen.yml        # top-level tgen ansible playbook\n\u2514\u2500\u2500 ...             # other top-level ansible playbooks\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>For detailed instructions on using SpliDT in your testbed, please refer to our usage guide.</p> <p>To evaluate SpliDT performance against Megaflow cache with real-world workloads, please refer to our benchmarking guide.</p>"},{"location":"installation/#additional-documentation","title":"Additional Documentation","text":"<ul> <li>Getting Started</li> <li>Technical Details</li> <li>Performance Benchmarks</li> <li>Usage Instructions</li> </ul>"},{"location":"publications/","title":"Publications","text":"<p>arXiv Preprint 2025 SpliDT: Partitioned Decision Trees for Scalable Stateful Inference at Line Rate Murayyiam Parvez, Annus Zulfiqar, Sylee Beltiukov, Shir Landau Feibish, Walter Willinger, Arpit Gupta, Muhammad Shahbaz</p>"},{"location":"publications/#team","title":"Team","text":"<ul> <li>Murayyiam Parvez (Purdue University)</li> <li>Annus Zulfiqar (University of Michigan)</li> <li>Sylee Beltiukov (University of California, Santa Barbara)</li> <li>Shir Landau Feibish (Open University of Israel)</li> <li>Walter Willinger (NIKSUN, Inc.)</li> <li>Arpit Gupta (University of California, Santa Barbara)</li> <li>Muhammad Shahbaz (University of Michigan)</li> </ul>"},{"location":"publications/#technical-deepdive","title":"Technical Deepdive","text":"<p>We maintain detailed documentation about SpliDT:</p> <ul> <li>Getting Started and Usage</li> <li>Architecture and Implementation</li> <li>Performance Analysis</li> </ul>"},{"location":"publications/#contact-us","title":"Contact Us","text":"<p>For research-related queries or collaborations:</p> <ul> <li>Email: parvezm@purdue.edu, zulfiqaa@umich.edu</li> </ul>"},{"location":"technical-deepdive/","title":"Technical Deepdive","text":"<p>This document describes the technical architecture and implementation details of Gigaflow.</p>"},{"location":"technical-deepdive/#gigaflow-architecture","title":"Gigaflow Architecture","text":"<p>Gigaflow is built as a caching sub-system in the Open vSwitch.</p> Gigaflow cache in the Open vSwitch (OVS) <ul> <li>The input packet that misses the cache is sent to the userspace where its <code>flow</code> is extracted (header field set).</li> <li>The flow passes through the vSwitch pipeline and its traversal is collected.</li> <li>Then, the travesal mapper, based on disjointedness, captures pipeline-aware locality from this traversal and suggests suitable sub-traversals to be cached.</li> <li>Finally, Longest Traversal Matching (LTM) cache entries are generated and installed into the Gigaflow table.</li> </ul>"},{"location":"technical-deepdive/#pipeline-aware-locality","title":"Pipeline-Aware Locality","text":"<p>The following figure shows an example of how pipeline-aware locality captures unseen traversals by caching shared sub-traversals. We smartly partition the traversal and determine suitable sub-traversals that have a high likelihood of being reused in the future by other flows.</p> An example of Gigaflow cache in action with three packets"},{"location":"technical-deepdive/#disjointedness-property","title":"Disjointedness Property","text":"<p>Capturing pipeline-Aware locality is expensive (see the paper) but we observe the following: disjoint sub-traversals maximize the cross-product rule space in Gigaflow tables.  We implement a dynamic program to find disjoint sub-traversals in a given traversal and this algorithm processes the traversal of each cache miss without looking at how previous cache entries were generated.</p> Gigaflow captures pipeline-aware locality by capitalizing on field-level disjointedness in traversals"},{"location":"technical-deepdive/#mapping-traversals-to-gigaflow","title":"Mapping Traversals to Gigaflow","text":""},{"location":"technical-deepdive/#traversal-processing","title":"Traversal Processing","text":"<p>The map_traversal_to_gigaflow function is responsible for mapping traversals to Gigaflow tables. It performs the following steps:</p> <ol> <li>Cleanup: Cleans up mapping context before starting the mapping process.</li> <li>Find Available Tables: Determines available Gigaflow tables with sufficient space for mapping.</li> <li>Disjointedness Exploration: Implements the dynamic program to explore disjointedness across sub-traversals.</li> <li>Compose and Map Wildcards: Composes and maps the wildcards as cache entries to the Gigaflow tables.</li> </ol> lib/dpif-netdev.c<pre><code>void\nmap_traversal_to_gigaflow(struct gigaflow_xlate_context *gf_xlate_ctx,\n                          struct gigaflow_mapping_context *gf_map_ctx,\n                          struct gigaflow_perf_stats *gf_stats,\n                          struct pmd_perf_stats *stats)\n{\n\n    /* no need to map if there was an error in translation \n       this happens for recursion too deep during resubmissions */\n    if (gf_xlate_ctx-&gt;xlate_error != XLATE_OK) {\n        gf_map_ctx-&gt;map_out = get_empty_mapper_out();\n        gf_map_ctx-&gt;mapped_cnt = 0;\n        return;\n    }\n\n    /* cleanup contexts before we start mapping \n       THIS MUST BE THE FIRST STEP WHEN WE START MAPPING */\n    cleanup_before_mapping(gf_xlate_ctx, gf_map_ctx);\n\n    /* 1. optimization passes to reduce the number of tables \n          to consider before calling path maximization DP\n        a. decoupling points in traversal?\n        b. header space locality opportunity?\n        c. explore vs exploit? */\n\n    /* generate bit wildcards and determine available tables for mapping */\n    generate_bit_wildcards(gf_xlate_ctx, gf_map_ctx);\n    find_available_tables_for_mapping(gf_map_ctx, gf_stats);\n\n    /* 2. find and assign a traversal mapping to Gigaflow tables \n       by maximizing added paths, coupling and other objectives */\n    uint64_t optimizer_cycles = cycles_counter_update(stats);\n    struct mapper_out *opt_map_out = maximize_optimality(gf_xlate_ctx, \n                                                         gf_map_ctx);\n    optimizer_cycles = cycles_counter_update(stats) - optimizer_cycles;\n    gf_perf_update_counter(gf_stats, GF_STAT_OPTIMIZER_CYCLES, optimizer_cycles);\n    gigaflow_mapping_ctx_assign_mapping(gf_xlate_ctx, gf_map_ctx, opt_map_out);\n\n    /* 3. compose and map the wildcards as Gigaflow entries to the accel */\n    uint64_t compose_cycles = cycles_counter_update(stats);\n    compose_gigaflows(gf_xlate_ctx, gf_map_ctx);\n    compose_cycles = cycles_counter_update(stats) - compose_cycles;\n    gf_perf_update_counter(gf_stats, GF_STAT_COMPOSITION_CYCLES, compose_cycles);\n\n    /* 4. accept this mapping iff masks are not exceeded in any Gigaflow table */\n    accept_mapping_if_masks_within_limits(gf_map_ctx, gf_stats);\n\n    const bool estimate_flow_space = \n        gf_map_ctx-&gt;gf_config-&gt;estimate_flow_space;\n    if (estimate_flow_space) {\n        /* 5. add this new mapping to our unique mappings */\n        gigaflow_state_add_new_mapping(gf_xlate_ctx, gf_map_ctx, gf_stats);\n    }\n}\n</code></pre>"},{"location":"technical-deepdive/#exploring-sub-traversals","title":"Exploring Sub-Traversals","text":"<p>The function maximize_optimality_dp implements the disjoint partitioning algorithm as a dynamic program to find the optimal mapping of traversals to Gigaflow tables.  It uses memoization to store intermediate results and avoid redundant calculations.</p> lib/mapper.c<pre><code>struct mapper_out* \nmaximize_optimality_dp(struct mapper_memo *memo,\n                       struct gigaflow_xlate_context *gf_xlate_ctx,\n                       struct gigaflow_mapping_context *gf_map_ctx,\n                       int t_start, int t_end, int g_start, int g_end)\n{\n    /* check memo before making any calls */\n    struct dp_memo_entry *dp_cache;\n    dp_cache = search_dp_memo(memo, t_start, t_end, g_start, g_end);\n    if (dp_cache) {\n        return dp_cache-&gt;map_out;\n    }\n    struct mapper_out *map_out = get_empty_mapper_out();\n    /* base case - 1: \n       no more traversal tables to map? */\n    if (t_end == -1) {\n        map_out-&gt;score = 0;\n        // nothing to map..\n        return map_out;\n    }\n    /* base case - 2:\n       only one Gigaflow table left?\n       map remaining traversal tables to this Gigaflow table */\n    if (g_end == 0) {\n        /* get actual Gigaflow table ID before measuring optimality\n           or updating a mapping on that table */\n        int gf_table_id = gf_map_ctx-&gt;available_tables[g_end];\n        int score = get_mapping_optimality(memo, gf_xlate_ctx, gf_map_ctx, \n                                           t_start+1, t_end, t_end+1, \n                                           gf_table_id);\n        map_out-&gt;score = score;\n        // update this path mapping in mapper_out\n        mapper_out_update_mapping(map_out, t_start+1, t_end, t_end+1, \n                                  gf_table_id);\n        // insert dp result into memo before returning\n        insert_into_dp_memo(memo, map_out, t_start, t_end, \n                            g_start, g_end, score);\n        return map_out;\n    }\n    // recursive calls\n    struct mapper_out *dp_map_out;\n    int mapped_score = 0, new_score = 0, max_score = 0;\n    for (int t_i=t_start; t_i&lt;=t_end; t_i++) {\n        dp_map_out = maximize_optimality_dp(memo, gf_xlate_ctx, gf_map_ctx, \n                                            t_start, t_i, g_start, \n                                            g_end-1);\n        /* get actual Gigaflow table ID before measuring optimality\n           or updating a mapping on that table */\n        int gf_table_id = gf_map_ctx-&gt;available_tables[g_end];\n        mapped_score = get_mapping_optimality(memo, gf_xlate_ctx, gf_map_ctx, \n                                              t_i+1, t_end, t_end+1, \n                                              gf_table_id);\n        new_score = dp_map_out-&gt;score + mapped_score;\n        /* found a better or equally optimal solution? \n           replace the prior mapping for this table */\n        if (new_score &gt; max_score) {\n            // free previous map_out and update to new best\n            mapper_out_destroy(map_out);\n            // update this path mapping in mapper_out iff t_i+1 &lt;= t_end\n            if (t_i + 1 &lt;= t_end) {\n                mapper_out_update_mapping(dp_map_out, t_i+1, t_end, \n                                          t_end+1, gf_table_id);\n            }\n            dp_map_out-&gt;score = new_score;\n            map_out = dp_map_out;\n            max_score = new_score;\n        } else {\n            // this solution we tried is not any better\n            mapper_out_destroy(dp_map_out);\n        }\n    }\n    // insert dp result into memo before returning\n    insert_into_dp_memo(memo, map_out, t_start, t_end, \n                        g_start, g_end, max_score);\n    return map_out;\n}\n</code></pre>"},{"location":"technical-deepdive/#evaluating-a-sub-traversal","title":"Evaluating a Sub-Traversal","text":"<p>The function that evaluates one sub-traversal to see if all tables overlap is called get_coupling.</p> lib/mapper.c<pre><code>int \nget_coupling(struct gigaflow_mapping_context *gf_map_ctx,\n             int t_start, int t_end)\n{\n    const uint32_t coupling_base_score = \n        gf_map_ctx-&gt;gf_config-&gt;coupling_base_score;\n    int coupling = 0; // just one wildcard is not a coupling\n    uint16_t next_wc_bits = EMPTY_WC;\n    uint16_t curr_wc_bits = gf_map_ctx-&gt;bit_wildcards[t_start];\n    for (int i=t_start+1; i&lt;=t_end; i++) {\n        next_wc_bits = gf_map_ctx-&gt;bit_wildcards[i];\n        /* if either of current or next are empty wildcards OR \n           if not, have common bits, subsume into current and move on */\n        if ((curr_wc_bits == EMPTY_WC) || (next_wc_bits == EMPTY_WC)\n            || (curr_wc_bits &amp; next_wc_bits)) {\n            curr_wc_bits |= next_wc_bits;\n            coupling += coupling_base_score; // better coupling\n        } else {\n            /* found decoupling within this range; bad combination */\n            return 0;\n        }\n    }\n    return coupling;\n}\n</code></pre>"},{"location":"technical-deepdive/#putting-it-all-together","title":"Putting It All Together","text":"<p>For a given traversal, our disjoint partitioning algorithm tries all sub-traversal combinations and evaluates them for disjointedness using the get_coupling function. The sub-traversal combination with maximum sum of disjointedness is selected for mapping to the Gigaflow tables.</p>"},{"location":"usage/","title":"Using SpliDT","text":"<p>This guide explains how to use SpliDT, a partitioned decision tree training and inference framework designed for scalable, line-rate deployment.</p>"},{"location":"usage/#prerequisites","title":"Prerequisites","text":"<p>The high-level SpliDT framework is shown in the figure below.  It includes the following components:</p> Workflow of SpliDT Model Design Search <p>We provide seven datasets hosted on Figshare for training and evaluating SpliDT. More details about the pipelines and datasets can be found in the installation and benchmarking sections.</p>"},{"location":"usage/#running-splidt-and-performance-evaluation","title":"Running SpliDT and Performance Evaluation","text":"<p>To evaluate performance:</p> <ul> <li>Refer to our benchmarking guide for instructions on evaluating SpliDT against baseline approaches on real-world security datasets</li> </ul>"},{"location":"usage/#next-steps","title":"Next Steps","text":"<ul> <li>See Installation Instructions for setup details</li> <li>Review Technical Details for architecture information</li> <li>Check Benchmarks for performance evaluation guide</li> </ul>"}]}